{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 661,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import openpyxl\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "chat_words_str = \"\"\"\n",
    "DM=Direct message\n",
    "CT=Cuttweet\n",
    "RT=Retweet\n",
    "PRT=Partial retweet\n",
    "MT=Modified tweet\n",
    "PRT=Please retweet\n",
    "HT=Hat tip\n",
    "CC=Carbon-copy\n",
    "CX=Correction\n",
    "EM=Email Marketing\n",
    "SEO=Search Engine Optimization\n",
    "SROI=Social Return on Investment\n",
    "SN=Social Network\n",
    "YT=YouTube\n",
    "UGC=User-Generated Content\n",
    "SMO=Social Media Optimization\n",
    "FB=Facebook\n",
    "LI=LinkedIn\n",
    "SM=Social Media\n",
    "SMM=Social Media Marketing\n",
    "EZine=Electronic Magazine\n",
    "BGD=Background\n",
    "CD9=Code 9, parents are around\n",
    "BTW=By the way\n",
    "AB=About\n",
    "ABT=About\n",
    "DD=Dear daughter\n",
    "AFAIK=far as I know\n",
    "AYFKMWTS=Are you f—ing kidding me with this s—?\n",
    "BR=Best regards\n",
    "CHK=Check\n",
    "CUL8R=See you later\n",
    "DP=used to mean “profile pic”\n",
    "FML=F— my life\n",
    "FUBAR=F—ed up beyond all repair (slang from the US Military)\n",
    "BBFN=Bye for now\n",
    "B4=Before\n",
    "DS=Dear son\n",
    "FF=Follow Friday\n",
    "EMA=Email address\n",
    "DYK=Do you know\n",
    "F2F=Face to face\n",
    "FTF=Face to face\n",
    "HAGN=Have a good night\n",
    "DF=Dear fiancé\n",
    "DAM=Don’t annoy me\n",
    "FFS=For F—‘s Sake\n",
    "EM=Email\n",
    "EML=Email\n",
    "FOTD=Find of the day\n",
    "FTW=For the win, F— the world\n",
    "FWIW=For what it’s worth\n",
    "HTH=Hope that helps\n",
    "GMAFB=Give me a f—ing break\n",
    "HAND=Have a nice day\n",
    "ICYMI=In case you missed it\n",
    "GTFOOH=Get the f— out of here\n",
    "GTS=Guess the song\n",
    "HOTD=Headline of the day\n",
    "IIRC=If I remember correctly\n",
    "KYSO=Knock your socks off\n",
    "KK=Kewl kewl, or ok, got it\n",
    "HT=Head through\n",
    "IC=I see\n",
    "IDK=I don’t know\n",
    "LHH=hella hard\n",
    "ZOMG=OMG to the max\n",
    "IMHO=In my humble opinion\n",
    "NFW=No f—ing way\n",
    "ORLY=Oh, really?\n",
    "YOYO=You’re on your own\n",
    "LMAO=Laughing my ass off\n",
    "IRL=In real life\n",
    "JK=Just kidding\n",
    "JV=Joint venture\n",
    "LO=Little One\n",
    "LOL=Laugh out loud\n",
    "MM=Music Monday\n",
    "LMK=Let me know\n",
    "TY=Thank you\n",
    "SRS=Serious\n",
    "STF=Shut the f—\n",
    "STFU=Shut the f— up!\n",
    "TL=Timeline\n",
    "TYIA=Thank you in advance\n",
    "TT=Trending topic\n",
    "TYVW=Thank you very much\n",
    "BRB=Be Right Back\n",
    "AFAIK=As Far As I Know\n",
    "AFK=Away From Keyboard\n",
    "ASAP=As Soon As Possible\n",
    "ATK=At The Keyboard\n",
    "ATM=At The Moment\n",
    "A3=Anytime, Anywhere, Anyplace\n",
    "BAK=Back At Keyboard\n",
    "BBL=Be Back Later\n",
    "BBS=Be Back Soon\n",
    "BFN=Bye For Now\n",
    "B4N=Bye For Now\n",
    "BRB=Be Right Back\n",
    "BRT=Be Right There\n",
    "BTW=By The Way\n",
    "B4=Before\n",
    "B4N=Bye For Now\n",
    "CU=See You\n",
    "CUL8R=See You Later\n",
    "CYA=See You\n",
    "FAQ=Frequently Asked Questions\n",
    "FC=Fingers Crossed\n",
    "FWIW=For What It's Worth\n",
    "FYI=For Your Information\n",
    "GAL=Get A Life\n",
    "GG=Good Game\n",
    "GN=Good Night\n",
    "GMTA=Great Minds Think Alike\n",
    "GR8=Great!\n",
    "G9=Genius\n",
    "IC=I See\n",
    "ICQ=I Seek you (also a chat program)\n",
    "ILU=ILU: I Love You\n",
    "IMHO=In My Honest Opinion\n",
    "IMO=In My Opinion\n",
    "IOW=In Other Words\n",
    "IRL=In Real Life\n",
    "KISS=Keep It Simple, Stupid\n",
    "LDR=Long Distance Relationship\n",
    "LMAO=Laugh My A.. Off\n",
    "LOL=Laughing Out Loud\n",
    "LTNS=Long Time No See\n",
    "L8R=Later\n",
    "MTE=My Thoughts Exactly\n",
    "M8=Mate\n",
    "NRN=No Reply Necessary\n",
    "OIC=Oh I See\n",
    "PITA=Pain In The A..\n",
    "PRT=Party\n",
    "SFW=Safe for work\n",
    "TY=Thank you\n",
    "TMB=Tweet me back\n",
    "BRB=Be Right Back\n",
    "IMO=In My Opinion\n",
    "RLRT=Real-life re-tweet, a close cousin to OH\n",
    "OOMF=One of my friends/followers\n",
    "NTS=Note to self\n",
    "RTFM=Read the f—ing manual\n",
    "SNAFU=Situation normal, all f—ed up (slang from the US Military)\n",
    "RLRT=Real-life re-tweet, a close cousin to OH\n",
    "SMH=Shaking my head\n",
    "STFW=Search the f—ing web!\n",
    "TFTT=Thanks for this tweet\n",
    "SOB=Son of a B—-\n",
    "TFTF=Thanks for the follow\n",
    "PRW=Parents Are Watching\n",
    "ROFL=Rolling On The Floor Laughing\n",
    "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
    "ROTFLMAO=Rolling On The Floor Laughing My A.. Off\n",
    "SK8=Skate\n",
    "STATS=Your sex and age\n",
    "ASL=Age, Sex, Location\n",
    "THX=Thank You\n",
    "TTFN=Ta-Ta For Now!\n",
    "TTYL=Talk To You Later\n",
    "U=You\n",
    "U2=You Too\n",
    "U4E=Yours For Ever\n",
    "WB=Welcome Back\n",
    "WTF=What The F...\n",
    "WTG=Way To Go!\n",
    "WUF=Where Are You From?\n",
    "W8=Wait...\n",
    "7K=Sick:-D Laugher\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"template.xlsx\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chat Words Conversion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "outputs": [
    {
     "data": {
      "text/plain": "                                       text  \\\n0      The cars drive, fast and erratically   \n1                      the car dsrive slow?   \n2  The bike ride very fast and erratically!   \n\n                      text_chat_conversion  \n0      The cars drive fast and erratically  \n1                       the car drive slow  \n2  The bike ride very fast and erratically  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>text_chat_conversion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The cars drive, fast and erratically</td>\n      <td>The cars drive fast and erratically</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>the car dsrive slow?</td>\n      <td>the car drive slow</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The bike ride very fast and erratically!</td>\n      <td>The bike ride very fast and erratically</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_words_map_dict = {}\n",
    "chat_words_list = []\n",
    "for line in chat_words_str.split(\"\\n\"):\n",
    "    if line != \"\":\n",
    "        cw = line.split(\"=\")[0]\n",
    "        cw_expanded = line.split(\"=\")[1]\n",
    "        chat_words_list.append(cw)\n",
    "        chat_words_map_dict[cw] = cw_expanded\n",
    "chat_words_list = set(chat_words_list)\n",
    "\n",
    "\n",
    "def chat_words_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words_list:\n",
    "            new_text.append(chat_words_map_dict[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "\n",
    "df[\"text_chat_conversion\"] = df[\"text\"].apply(lambda text: correct_spellings(text))\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Spelling correction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "\n",
    "\n",
    "df[\"correct_spellings\"] = df[\"text_chat_conversion\"].apply(lambda text: correct_spellings(text))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Punctuation removal"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "outputs": [],
   "source": [
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "\n",
    "\n",
    "def remove_punctation(text):\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "\n",
    "df[\"text_punctation_remove\"] = df[\"correct_spellings\"].apply(lambda text: remove_punctation(text))\n",
    "df[\"text_lower\"] = df[\"text_punctation_remove\"].str.lower()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stop Words removal"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "outputs": [
    {
     "data": {
      "text/plain": "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\", \".join(stopwords.words('english'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "\n",
    "df[\"text_stopwords_removal\"] = df[\"text_lower\"].apply(lambda text: remove_stopwords(text))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Freq and rare words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# cnt = Counter()\n",
    "# for text in df[\"text_stopwords_removal\"].values:\n",
    "#     for word in text.split():\n",
    "#         cnt[word] += 1\n",
    "#\n",
    "# cnt.most_common(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "outputs": [],
   "source": [
    "# FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n",
    "# def remove_freqwords(text):\n",
    "#     \"\"\"custom function to remove the frequent words\"\"\"\n",
    "#     return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
    "#\n",
    "# df[\"text_wo_stopfreq\"] = df[\"text_wo_stop\"].apply(lambda text: remove_freqwords(text))\n",
    "# df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "outputs": [],
   "source": [
    "# n_rare_words = 10\n",
    "# RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
    "# def remove_rarewords(text):\n",
    "#     \"\"\"custom function to remove the rare words\"\"\"\n",
    "#     return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n",
    "#\n",
    "# df[\"text_wo_stopfreqrare\"] = df[\"text_wo_stopfreq\"].apply(lambda text: remove_rarewords(text))\n",
    "# df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stemming"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Drop the two columns\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def stem_words(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "\n",
    "df[\"text_stemmed\"] = df[\"text_stopwords_removal\"].apply(lambda text: stem_words(text))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "\n",
    "df[\"text_preprocessed\"] = df[\"text_stopwords_removal\"].apply(lambda text: lemmatize_words(text))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "outputs": [
    {
     "data": {
      "text/plain": "                                       text  \\\n0      The cars drive, fast and erratically   \n1                      the car dsrive slow?   \n2  The bike ride very fast and erratically!   \n\n                      text_chat_conversion  \\\n0      The cars drive fast and erratically   \n1                       the car drive slow   \n2  The bike ride very fast and erratically   \n\n                         correct_spellings  \\\n0      The cars drive fast and erratically   \n1                       the car drive slow   \n2  The bike ride very fast and erratically   \n\n                    text_punctation_remove  \\\n0      The cars drive fast and erratically   \n1                       the car drive slow   \n2  The bike ride very fast and erratically   \n\n                                text_lower       text_stopwords_removal  \\\n0      the cars drive fast and erratically  cars drive fast erratically   \n1                       the car drive slow               car drive slow   \n2  the bike ride very fast and erratically   bike ride fast erratically   \n\n           text_stemmed           text_preprocessed  \\\n0  car drive fast errat  car drive fast erratically   \n1        car drive slow              car drive slow   \n2  bike ride fast errat  bike ride fast erratically   \n\n                  text_lemmatized_with_pos  \n0       The car drive fast and erratically  \n1                       the car drive slow  \n2  The bike ride very fast and erratically  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>text_chat_conversion</th>\n      <th>correct_spellings</th>\n      <th>text_punctation_remove</th>\n      <th>text_lower</th>\n      <th>text_stopwords_removal</th>\n      <th>text_stemmed</th>\n      <th>text_preprocessed</th>\n      <th>text_lemmatized_with_pos</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The cars drive, fast and erratically</td>\n      <td>The cars drive fast and erratically</td>\n      <td>The cars drive fast and erratically</td>\n      <td>The cars drive fast and erratically</td>\n      <td>the cars drive fast and erratically</td>\n      <td>cars drive fast erratically</td>\n      <td>car drive fast errat</td>\n      <td>car drive fast erratically</td>\n      <td>The car drive fast and erratically</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>the car dsrive slow?</td>\n      <td>the car drive slow</td>\n      <td>the car drive slow</td>\n      <td>the car drive slow</td>\n      <td>the car drive slow</td>\n      <td>car drive slow</td>\n      <td>car drive slow</td>\n      <td>car drive slow</td>\n      <td>the car drive slow</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The bike ride very fast and erratically!</td>\n      <td>The bike ride very fast and erratically</td>\n      <td>The bike ride very fast and erratically</td>\n      <td>The bike ride very fast and erratically</td>\n      <td>the bike ride very fast and erratically</td>\n      <td>bike ride fast erratically</td>\n      <td>bike ride fast errat</td>\n      <td>bike ride fast erratically</td>\n      <td>The bike ride very fast and erratically</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"J\": wordnet.ADJ, \"R\": wordnet.ADV}\n",
    "\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join(\n",
    "        [lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "\n",
    "\n",
    "df[\"text_lemmatized_with_pos\"] = df[\"correct_spellings\"].apply(lambda text: lemmatize_words(text))\n",
    "df.head()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "outputs": [],
   "source": [
    "## Emoji and emoticons removal/conversion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "outputs": [],
   "source": [
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "# def remove_emoji(string):\n",
    "#     emoji_pattern = re.compile(\"[\"\n",
    "#                            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#                            u\"\\U00002702-\\U000027B0\"\n",
    "#                            u\"\\U000024C2-\\U0001F251\"\n",
    "#                            \"]+\", flags=re.UNICODE)\n",
    "#     return emoji_pattern.sub(r'', string)\n",
    "# df[\"text_no_emoji\"] = df[\"text_lemmatized_with_pos\"].apply(lambda text: remove_emoji(text))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "outputs": [],
   "source": [
    "# Thanks : https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py\n",
    "EMOTICONS = {\n",
    "    u\":‑\\)\": \"Happy face or smiley\",\n",
    "    u\":\\)\": \"Happy face or smiley\",\n",
    "    u\":-\\]\": \"Happy face or smiley\",\n",
    "    u\":\\]\": \"Happy face or smiley\",\n",
    "    u\":-3\": \"Happy face smiley\",\n",
    "    u\":3\": \"Happy face smiley\",\n",
    "    u\":->\": \"Happy face smiley\",\n",
    "    u\":>\": \"Happy face smiley\",\n",
    "    u\"8-\\)\": \"Happy face smiley\",\n",
    "    u\":o\\)\": \"Happy face smiley\",\n",
    "    u\":-\\}\": \"Happy face smiley\",\n",
    "    u\":\\}\": \"Happy face smiley\",\n",
    "    u\":-\\)\": \"Happy face smiley\",\n",
    "    u\":c\\)\": \"Happy face smiley\",\n",
    "    u\":\\^\\)\": \"Happy face smiley\",\n",
    "    u\"=\\]\": \"Happy face smiley\",\n",
    "    u\"=\\)\": \"Happy face smiley\",\n",
    "    u\":‑D\": \"Laughing, big grin or laugh with glasses\",\n",
    "    u\":D\": \"Laughing, big grin or laugh with glasses\",\n",
    "    u\"8‑D\": \"Laughing, big grin or laugh with glasses\",\n",
    "    u\"8D\": \"Laughing, big grin or laugh with glasses\",\n",
    "    u\"X‑D\": \"Laughing, big grin or laugh with glasses\",\n",
    "    u\"XD\": \"Laughing, big grin or laugh with glasses\",\n",
    "    u\"=D\": \"Laughing, big grin or laugh with glasses\",\n",
    "    u\"=3\": \"Laughing, big grin or laugh with glasses\",\n",
    "    u\"B\\^D\": \"Laughing, big grin or laugh with glasses\",\n",
    "    u\":-\\)\\)\": \"Very happy\",\n",
    "    u\":‑\\(\": \"Frown, sad, andry or pouting\",\n",
    "    u\":-\\(\": \"Frown, sad, andry or pouting\",\n",
    "    u\":\\(\": \"Frown, sad, andry or pouting\",\n",
    "    u\":‑c\": \"Frown, sad, andry or pouting\",\n",
    "    u\":c\": \"Frown, sad, andry or pouting\",\n",
    "    u\":‑<\": \"Frown, sad, andry or pouting\",\n",
    "    u\":<\": \"Frown, sad, andry or pouting\",\n",
    "    u\":‑\\[\": \"Frown, sad, andry or pouting\",\n",
    "    u\":\\[\": \"Frown, sad, andry or pouting\",\n",
    "    u\":-\\|\\|\": \"Frown, sad, andry or pouting\",\n",
    "    u\">:\\[\": \"Frown, sad, andry or pouting\",\n",
    "    u\":\\{\": \"Frown, sad, andry or pouting\",\n",
    "    u\":@\": \"Frown, sad, andry or pouting\",\n",
    "    u\">:\\(\": \"Frown, sad, andry or pouting\",\n",
    "    u\":'‑\\(\": \"Crying\",\n",
    "    u\":'\\(\": \"Crying\",\n",
    "    u\":'‑\\)\": \"Tears of happiness\",\n",
    "    u\":'\\)\": \"Tears of happiness\",\n",
    "    u\"D‑':\": \"Horror\",\n",
    "    u\"D:<\": \"Disgust\",\n",
    "    u\"D:\": \"Sadness\",\n",
    "    u\"D8\": \"Great dismay\",\n",
    "    u\"D;\": \"Great dismay\",\n",
    "    u\"D=\": \"Great dismay\",\n",
    "    u\"DX\": \"Great dismay\",\n",
    "    u\":‑O\": \"Surprise\",\n",
    "    u\":O\": \"Surprise\",\n",
    "    u\":‑o\": \"Surprise\",\n",
    "    u\":o\": \"Surprise\",\n",
    "    u\":-0\": \"Shock\",\n",
    "    u\"8‑0\": \"Yawn\",\n",
    "    u\">:O\": \"Yawn\",\n",
    "    u\":-\\*\": \"Kiss\",\n",
    "    u\":\\*\": \"Kiss\",\n",
    "    u\":X\": \"Kiss\",\n",
    "    u\";‑\\)\": \"Wink or smirk\",\n",
    "    u\";\\)\": \"Wink or smirk\",\n",
    "    u\"\\*-\\)\": \"Wink or smirk\",\n",
    "    u\"\\*\\)\": \"Wink or smirk\",\n",
    "    u\";‑\\]\": \"Wink or smirk\",\n",
    "    u\";\\]\": \"Wink or smirk\",\n",
    "    u\";\\^\\)\": \"Wink or smirk\",\n",
    "    u\":‑,\": \"Wink or smirk\",\n",
    "    u\";D\": \"Wink or smirk\",\n",
    "    u\":‑P\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":P\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"X‑P\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"XP\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":‑Þ\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":Þ\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":b\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"d:\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"=p\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\">:P\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":‑/\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":/\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":-[.]\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\">:[(\\\\\\)]\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\">:/\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":[(\\\\\\)]\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=/\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=[(\\\\\\)]\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":L\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=L\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":S\": \"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":‑\\|\": \"Straight face\",\n",
    "    u\":\\|\": \"Straight face\",\n",
    "    u\":$\": \"Embarrassed or blushing\",\n",
    "    u\":‑x\": \"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":x\": \"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":‑#\": \"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":#\": \"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":‑&\": \"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":&\": \"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\"O:‑\\)\": \"Angel, saint or innocent\",\n",
    "    u\"O:\\)\": \"Angel, saint or innocent\",\n",
    "    u\"0:‑3\": \"Angel, saint or innocent\",\n",
    "    u\"0:3\": \"Angel, saint or innocent\",\n",
    "    u\"0:‑\\)\": \"Angel, saint or innocent\",\n",
    "    u\"0:\\)\": \"Angel, saint or innocent\",\n",
    "    u\":‑b\": \"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"0;\\^\\)\": \"Angel, saint or innocent\",\n",
    "    u\">:‑\\)\": \"Evil or devilish\",\n",
    "    u\">:\\)\": \"Evil or devilish\",\n",
    "    u\"\\}:‑\\)\": \"Evil or devilish\",\n",
    "    u\"\\}:\\)\": \"Evil or devilish\",\n",
    "    u\"3:‑\\)\": \"Evil or devilish\",\n",
    "    u\"3:\\)\": \"Evil or devilish\",\n",
    "    u\">;\\)\": \"Evil or devilish\",\n",
    "    u\"\\|;‑\\)\": \"Cool\",\n",
    "    u\"\\|‑O\": \"Bored\",\n",
    "    u\":‑J\": \"Tongue-in-cheek\",\n",
    "    u\"#‑\\)\": \"Party all night\",\n",
    "    u\"%‑\\)\": \"Drunk or confused\",\n",
    "    u\"%\\)\": \"Drunk or confused\",\n",
    "    u\":-###..\": \"Being sick\",\n",
    "    u\":###..\": \"Being sick\",\n",
    "    u\"<:‑\\|\": \"Dump\",\n",
    "    u\"\\(>_<\\)\": \"Troubled\",\n",
    "    u\"\\(>_<\\)>\": \"Troubled\",\n",
    "    u\"\\(';'\\)\": \"Baby\",\n",
    "    u\"\\(\\^\\^>``\": \"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(\\^_\\^;\\)\": \"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(-_-;\\)\": \"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(~_~;\\) \\(・\\.・;\\)\": \"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(-_-\\)zzz\": \"Sleeping\",\n",
    "    u\"\\(\\^_-\\)\": \"Wink\",\n",
    "    u\"\\(\\(\\+_\\+\\)\\)\": \"Confused\",\n",
    "    u\"\\(\\+o\\+\\)\": \"Confused\",\n",
    "    u\"\\(o\\|o\\)\": \"Ultraman\",\n",
    "    u\"\\^_\\^\": \"Joyful\",\n",
    "    u\"\\(\\^_\\^\\)/\": \"Joyful\",\n",
    "    u\"\\(\\^O\\^\\)／\": \"Joyful\",\n",
    "    u\"\\(\\^o\\^\\)／\": \"Joyful\",\n",
    "    u\"\\(__\\)\": \"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"_\\(\\._\\.\\)_\": \"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"<\\(_ _\\)>\": \"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"<m\\(__\\)m>\": \"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"m\\(__\\)m\": \"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"m\\(_ _\\)m\": \"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"\\('_'\\)\": \"Sad or Crying\",\n",
    "    u\"\\(/_;\\)\": \"Sad or Crying\",\n",
    "    u\"\\(T_T\\) \\(;_;\\)\": \"Sad or Crying\",\n",
    "    u\"\\(;_;\": \"Sad of Crying\",\n",
    "    u\"\\(;_:\\)\": \"Sad or Crying\",\n",
    "    u\"\\(;O;\\)\": \"Sad or Crying\",\n",
    "    u\"\\(:_;\\)\": \"Sad or Crying\",\n",
    "    u\"\\(ToT\\)\": \"Sad or Crying\",\n",
    "    u\";_;\": \"Sad or Crying\",\n",
    "    u\";-;\": \"Sad or Crying\",\n",
    "    u\";n;\": \"Sad or Crying\",\n",
    "    u\";;\": \"Sad or Crying\",\n",
    "    u\"Q\\.Q\": \"Sad or Crying\",\n",
    "    u\"T\\.T\": \"Sad or Crying\",\n",
    "    u\"QQ\": \"Sad or Crying\",\n",
    "    u\"Q_Q\": \"Sad or Crying\",\n",
    "    u\"\\(-\\.-\\)\": \"Shame\",\n",
    "    u\"\\(-_-\\)\": \"Shame\",\n",
    "    u\"\\(一一\\)\": \"Shame\",\n",
    "    u\"\\(；一_一\\)\": \"Shame\",\n",
    "    u\"\\(=_=\\)\": \"Tired\",\n",
    "    u\"\\(=\\^\\·\\^=\\)\": \"cat\",\n",
    "    u\"\\(=\\^\\·\\·\\^=\\)\": \"cat\",\n",
    "    u\"=_\\^=\t\": \"cat\",\n",
    "    u\"\\(\\.\\.\\)\": \"Looking down\",\n",
    "    u\"\\(\\._\\.\\)\": \"Looking down\",\n",
    "    u\"\\^m\\^\": \"Giggling with hand covering mouth\",\n",
    "    u\"\\(\\・\\・?\": \"Confusion\",\n",
    "    u\"\\(?_?\\)\": \"Confusion\",\n",
    "    u\">\\^_\\^<\": \"Normal Laugh\",\n",
    "    u\"<\\^!\\^>\": \"Normal Laugh\",\n",
    "    u\"\\^/\\^\": \"Normal Laugh\",\n",
    "    u\"\\（\\*\\^_\\^\\*）\": \"Normal Laugh\",\n",
    "    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\": \"Normal Laugh\",\n",
    "    u\"\\(^\\^\\)\": \"Normal Laugh\",\n",
    "    u\"\\(\\^\\.\\^\\)\": \"Normal Laugh\",\n",
    "    u\"\\(\\^_\\^\\.\\)\": \"Normal Laugh\",\n",
    "    u\"\\(\\^_\\^\\)\": \"Normal Laugh\",\n",
    "    u\"\\(\\^\\^\\)\": \"Normal Laugh\",\n",
    "    u\"\\(\\^J\\^\\)\": \"Normal Laugh\",\n",
    "    u\"\\(\\*\\^\\.\\^\\*\\)\": \"Normal Laugh\",\n",
    "    u\"\\(\\^—\\^\\）\": \"Normal Laugh\",\n",
    "    u\"\\(#\\^\\.\\^#\\)\": \"Normal Laugh\",\n",
    "    u\"\\（\\^—\\^\\）\": \"Waving\",\n",
    "    u\"\\(;_;\\)/~~~\": \"Waving\",\n",
    "    u\"\\(\\^\\.\\^\\)/~~~\": \"Waving\",\n",
    "    u\"\\(-_-\\)/~~~ \\($\\·\\·\\)/~~~\": \"Waving\",\n",
    "    u\"\\(T_T\\)/~~~\": \"Waving\",\n",
    "    u\"\\(ToT\\)/~~~\": \"Waving\",\n",
    "    u\"\\(\\*\\^0\\^\\*\\)\": \"Excited\",\n",
    "    u\"\\(\\*_\\*\\)\": \"Amazed\",\n",
    "    u\"\\(\\*_\\*;\": \"Amazed\",\n",
    "    u\"\\(\\+_\\+\\) \\(@_@\\)\": \"Amazed\",\n",
    "    u\"\\(\\*\\^\\^\\)v\": \"Laughing,Cheerful\",\n",
    "    u\"\\(\\^_\\^\\)v\": \"Laughing,Cheerful\",\n",
    "    u\"\\(\\(d[-_-]b\\)\\)\": \"Headphones,Listening to music\",\n",
    "    u'\\(-\"-\\)': \"Worried\",\n",
    "    u\"\\(ーー;\\)\": \"Worried\",\n",
    "    u\"\\(\\^0_0\\^\\)\": \"Eyeglasses\",\n",
    "    u\"\\(\\＾ｖ\\＾\\)\": \"Happy\",\n",
    "    u\"\\(\\＾ｕ\\＾\\)\": \"Happy\",\n",
    "    u\"\\(\\^\\)o\\(\\^\\)\": \"Happy\",\n",
    "    u\"\\(\\^O\\^\\)\": \"Happy\",\n",
    "    u\"\\(\\^o\\^\\)\": \"Happy\",\n",
    "    u\"\\)\\^o\\^\\(\": \"Happy\",\n",
    "    u\":O o_O\": \"Surprised\",\n",
    "    u\"o_0\": \"Surprised\",\n",
    "    u\"o\\.O\": \"Surpised\",\n",
    "    u\"\\(o\\.o\\)\": \"Surprised\",\n",
    "    u\"oO\": \"Surprised\",\n",
    "    u\"\\(\\*￣m￣\\)\": \"Dissatisfied\",\n",
    "    u\"\\(‘A`\\)\": \"Snubbed or Deflated\"\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "outputs": [
    {
     "data": {
      "text/plain": "'Hello Happy_face_smiley Happy_face_smiley'"
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'(' + emot + ')', \"_\".join(EMOTICONS[emot].replace(\",\", \"\").split()), text)\n",
    "    return text\n",
    "\n",
    "\n",
    "text = \"Hello :-) :-)\"\n",
    "convert_emoticons(text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "outputs": [],
   "source": [
    "#TODO EMOJI replace with pytyhon emoji library"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Remove urls and HTML"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizacja"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "\n",
    "def tokenize_tweets(text):\n",
    "    return tweet_tokenizer.tokenize(text)\n",
    "\n",
    "df[\"text_tokenize\"] = df[\"text_chat_conversion\"].apply(lambda text: tokenize_tweets(text))\n",
    "df[\"text_tokenize_preprocessed\"] = df[\"text_preprocessed\"].apply(lambda text: tokenize_tweets(text))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                                     0  \\\ntext                              The cars drive, fast and erratically   \ntext_chat_conversion               The cars drive fast and erratically   \ncorrect_spellings                  The cars drive fast and erratically   \ntext_punctation_remove             The cars drive fast and erratically   \ntext_lower                         the cars drive fast and erratically   \ntext_stopwords_removal                     cars drive fast erratically   \ntext_stemmed                                      car drive fast errat   \ntext_preprocessed                           car drive fast erratically   \ntext_lemmatized_with_pos            The car drive fast and erratically   \ntext_tokenize               [The, cars, drive, fast, and, erratically]   \ntext_tokenize_preprocessed             [car, drive, fast, erratically]   \n\n                                                  1  \\\ntext                           the car dsrive slow?   \ntext_chat_conversion             the car drive slow   \ncorrect_spellings                the car drive slow   \ntext_punctation_remove           the car drive slow   \ntext_lower                       the car drive slow   \ntext_stopwords_removal               car drive slow   \ntext_stemmed                         car drive slow   \ntext_preprocessed                    car drive slow   \ntext_lemmatized_with_pos         the car drive slow   \ntext_tokenize               [the, car, drive, slow]   \ntext_tokenize_preprocessed       [car, drive, slow]   \n\n                                                                          2  \ntext                               The bike ride very fast and erratically!  \ntext_chat_conversion                The bike ride very fast and erratically  \ncorrect_spellings                   The bike ride very fast and erratically  \ntext_punctation_remove              The bike ride very fast and erratically  \ntext_lower                          the bike ride very fast and erratically  \ntext_stopwords_removal                           bike ride fast erratically  \ntext_stemmed                                           bike ride fast errat  \ntext_preprocessed                                bike ride fast erratically  \ntext_lemmatized_with_pos            The bike ride very fast and erratically  \ntext_tokenize               [The, bike, ride, very, fast, and, erratically]  \ntext_tokenize_preprocessed                  [bike, ride, fast, erratically]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>text</th>\n      <td>The cars drive, fast and erratically</td>\n      <td>the car dsrive slow?</td>\n      <td>The bike ride very fast and erratically!</td>\n    </tr>\n    <tr>\n      <th>text_chat_conversion</th>\n      <td>The cars drive fast and erratically</td>\n      <td>the car drive slow</td>\n      <td>The bike ride very fast and erratically</td>\n    </tr>\n    <tr>\n      <th>correct_spellings</th>\n      <td>The cars drive fast and erratically</td>\n      <td>the car drive slow</td>\n      <td>The bike ride very fast and erratically</td>\n    </tr>\n    <tr>\n      <th>text_punctation_remove</th>\n      <td>The cars drive fast and erratically</td>\n      <td>the car drive slow</td>\n      <td>The bike ride very fast and erratically</td>\n    </tr>\n    <tr>\n      <th>text_lower</th>\n      <td>the cars drive fast and erratically</td>\n      <td>the car drive slow</td>\n      <td>the bike ride very fast and erratically</td>\n    </tr>\n    <tr>\n      <th>text_stopwords_removal</th>\n      <td>cars drive fast erratically</td>\n      <td>car drive slow</td>\n      <td>bike ride fast erratically</td>\n    </tr>\n    <tr>\n      <th>text_stemmed</th>\n      <td>car drive fast errat</td>\n      <td>car drive slow</td>\n      <td>bike ride fast errat</td>\n    </tr>\n    <tr>\n      <th>text_preprocessed</th>\n      <td>car drive fast erratically</td>\n      <td>car drive slow</td>\n      <td>bike ride fast erratically</td>\n    </tr>\n    <tr>\n      <th>text_lemmatized_with_pos</th>\n      <td>The car drive fast and erratically</td>\n      <td>the car drive slow</td>\n      <td>The bike ride very fast and erratically</td>\n    </tr>\n    <tr>\n      <th>text_tokenize</th>\n      <td>[The, cars, drive, fast, and, erratically]</td>\n      <td>[the, car, drive, slow]</td>\n      <td>[The, bike, ride, very, fast, and, erratically]</td>\n    </tr>\n    <tr>\n      <th>text_tokenize_preprocessed</th>\n      <td>[car, drive, fast, erratically]</td>\n      <td>[car, drive, slow]</td>\n      <td>[bike, ride, fast, erratically]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.T"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ANALIZA DANYCH\n",
    "\n",
    "## Bag of Words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bike', 'car', 'drive', 'erratically', 'fast', 'ride', 'slow']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=False)\n",
    "vectorizer.fit(df[\"text_preprocessed\"])\n",
    "print(sorted(vectorizer.vocabulary_))\n",
    "vector = vectorizer.transform(df[\"text_preprocessed\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 1 0 0]\n",
      " [0 1 1 0 0 0 1]\n",
      " [1 0 0 1 1 1 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "   bike  car  drive  erratically  fast  ride  slow\n0     0    1      1            1     1     0     0\n1     0    1      1            0     0     0     1\n2     1    0      0            1     1     1     0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bike</th>\n      <th>car</th>\n      <th>drive</th>\n      <th>erratically</th>\n      <th>fast</th>\n      <th>ride</th>\n      <th>slow</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 685,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vector.toarray())\n",
    "pd.DataFrame(vector.toarray(),columns=sorted(vectorizer.vocabulary_))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'and', 'bike', 'car', 'cars', 'drive', 'erratically', 'fast', 'ride', 'slow', 'the', 'very']\n"
     ]
    }
   ],
   "source": [
    "vectorizer.fit(df[\"text_punctation_remove\"])\n",
    "print(sorted(vectorizer.vocabulary_))\n",
    "vector = vectorizer.transform(df[\"text_punctation_remove\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 1 1 1 1 0 0 0 0]\n",
      " [0 0 0 1 0 1 0 0 0 1 1 0]\n",
      " [1 1 1 0 0 0 1 1 1 0 0 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "   The  and  bike  car  cars  drive  erratically  fast  ride  slow  the  very\n0    1    1     0    0     1      1            1     1     0     0    0     0\n1    0    0     0    1     0      1            0     0     0     1    1     0\n2    1    1     1    0     0      0            1     1     1     0    0     1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>The</th>\n      <th>and</th>\n      <th>bike</th>\n      <th>car</th>\n      <th>cars</th>\n      <th>drive</th>\n      <th>erratically</th>\n      <th>fast</th>\n      <th>ride</th>\n      <th>slow</th>\n      <th>the</th>\n      <th>very</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 687,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vector.toarray())\n",
    "pd.DataFrame(vector.toarray(),columns=sorted(vectorizer.vocabulary_))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
